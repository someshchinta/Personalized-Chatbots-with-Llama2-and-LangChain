{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Models\n",
    "\n",
    "- [TheBloke/wizardLM-7B-HF](https://huggingface.co/TheBloke/wizardLM-7B-HF)\n",
    "- [daryl149/llama-2-7b-chat-hf](https://huggingface.co/daryl149/llama-2-7b-chat-hf)\n",
    "- [daryl149/llama-2-13b-chat-hf](https://huggingface.co/daryl149/llama-2-13b-chat-hf)\n",
    "- [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-93de3be4-cfc1-edfb-5c00-d241f4dfdf77)\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.49 s, sys: 7.12 s, total: 11.6 s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "! pip install sentence_transformers==2.2.2\n",
    "\n",
    "! pip install -qq -U langchain\n",
    "! pip install -qq -U tiktoken\n",
    "! pip install -qq -U pypdf\n",
    "! pip install -qq -U faiss-gpu\n",
    "! pip install -qq -U InstructorEmbedding \n",
    "\n",
    "! pip install -qq -U transformers \n",
    "! pip install -qq -U accelerate\n",
    "! pip install -qq -U bitsandbytes\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain: 0.1.10\n",
      "torch: 2.2.1+cu121\n",
      "transformers: 4.38.2\n",
      "CPU times: user 4.49 s, sys: 1.24 s, total: 5.73 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "import langchain\n",
    "\n",
    "# loaders\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# splits\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# vector stores\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# retrievers\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print('langchain:', langchain.__version__)\n",
    "print('torch:', torch.__version__)\n",
    "print('transformers:', transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/HP_Books/Harry Potter - Book 1 - The Sorcerers Stone.pdf',\n",
       " '/data/HP_Books/Harry Potter - Book 2 - The Chamber of Secrets.pdf',\n",
       " '/data/HP_Books/Harry Potter - Book 3 - The Prisoner of Azkaban.pdf',\n",
       " '/data/HP_Books/Harry Potter - Book 4 - The Goblet of Fire.pdf',\n",
       " '/data/HP_Books/Harry Potter - Book 5 - The Order of the Phoenix.pdf',\n",
       " '/data/HP_Books/Harry Potter - Book 6 - The Half-Blood Prince.pdf',\n",
       " '/data/HP_Books/Harry Potter - Book 7 - The Deathly Hallows.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob.glob('/data/HP_Books/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG\n",
    "\n",
    "- CFG class enables easy and organized experimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # LLMs\n",
    "    model_name = 'llama2-13b-chat' # wizardlm, llama2-7b-chat, llama2-13b-chat, mistral-7B\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    repetition_penalty = 1.15    \n",
    "\n",
    "    # splitting\n",
    "    split_chunk_size = 800\n",
    "    split_overlap = 0\n",
    "    \n",
    "    # embeddings\n",
    "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'    \n",
    "\n",
    "    # similar passages\n",
    "    k = 3\n",
    "    \n",
    "    # paths\n",
    "    PDFs_path = '/data/HP_Books/'\n",
    "    Embeddings_path =  '/data/faiss-hp-sentence-transformers'\n",
    "    Persist_directory = './harry-potter-vectordb'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model = CFG.model_name):\n",
    "\n",
    "    print('\\nDownloading model: ', model, '\\n\\n')\n",
    "\n",
    "    if model == 'wizardlm':\n",
    "        model_repo = 'TheBloke/wizardLM-7B-HF'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit = True,\n",
    "            device_map = 'auto',\n",
    "            torch_dtype = torch.float16,\n",
    "            low_cpu_mem_usage = True\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "\n",
    "    elif model == 'llama2-7b-chat':\n",
    "        model_repo = 'daryl149/llama-2-7b-chat-hf'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit = True,\n",
    "            device_map = 'auto',\n",
    "            torch_dtype = torch.float16,\n",
    "            low_cpu_mem_usage = True,\n",
    "            trust_remote_code = True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048\n",
    "\n",
    "    elif model == 'llama2-13b-chat':\n",
    "        model_repo = 'daryl149/llama-2-13b-chat-hf'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit = True,        \n",
    "            device_map = 'auto',\n",
    "            torch_dtype = torch.float16,\n",
    "            low_cpu_mem_usage = True,\n",
    "            trust_remote_code = True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048 # 8192\n",
    "\n",
    "    elif model == 'mistral-7B':\n",
    "        model_repo = 'mistralai/Mistral-7B-v0.1'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit = True,\n",
    "            device_map = 'auto',\n",
    "            torch_dtype = torch.float16,\n",
    "            low_cpu_mem_usage = True,\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "\n",
    "    else:\n",
    "        print(\"Not implemented model (tokenizer and backbone)\")\n",
    "\n",
    "    return tokenizer, model, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading model:  llama2-13b-chat \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d98754ac4140559b24589e038a3a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='tokenizer_config.json'), FloatProgress(value=0.0, max=727.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a272a617bf144ecb4f49786417a1452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='tokenizer.model'), FloatProgress(value=0.0, max=499723.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b836ac78354a2680565fa3d17f89dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='tokenizer.json'), FloatProgress(value=0.0, max=1842665.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7d7b758407463cb17e2d4db2164193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='special_tokens_map.json'), FloatProgress(value=0.0, max=411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497af4d6f0b1431fab339ff57818f647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='config.json'), FloatProgress(value=0.0, max=507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0327e8157b419ea6492b693c302326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='pytorch_model.bin.index.json'), FloatProgress(value=0.0, max=33444.0), HTML(value='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61973b76b28b47069661caeba50b7bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading shards'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f221af8aa7d74fe8b11205c69af6511d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='pytorch_model-00001-of-00003.bin'), FloatProgress(value=0.0, max=9948728430.0), HTMâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e70b295875542fa8e353d22d750b305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='pytorch_model-00002-of-00003.bin'), FloatProgress(value=0.0, max=9904165024.0), HTMâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5355fcc3d94586b6197ced5e5541de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='pytorch_model-00003-of-00003.bin'), FloatProgress(value=0.0, max=6178983625.0), HTMâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7a297e85b242dea3c3190b266a00e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading checkpoint shards'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c784f0d3341842ceb50d44ec0bab9e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='generation_config.json'), FloatProgress(value=0.0, max=137.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 34.6 s, sys: 42.7 s, total: 1min 17s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer, model, max_len = get_model(model = CFG.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check how Accelerate split the model across the available devices (GPUs)\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— pipeline\n",
    "\n",
    "- Hugging Face pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_length = max_len,\n",
    "    temperature = CFG.temperature,\n",
    "    top_p = CFG.top_p,\n",
    "    repetition_penalty = CFG.repetition_penalty\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f3130386a30>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.2 s, sys: 908 ms, total: 52.1 s\n",
      "Wall time: 54.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\".\\n\\nSure thing! Here are five examples of cool potions in the world of Dungeons & Dragons, along with a brief description of their effects:\\n\\n1. Potion of Healing: This potion restores hit points to the drinker, healing wounds and injuries sustained during combat or other physical activities. It's a staple of many adventurers' inventories, as it can be used to recover from dangerous battles or long journeys.\\n2. Potion of Invisibility: As its name suggests, this potion grants the drinker temporary invisibility, allowing them to move undetected and strike from unexpected angles. It's often used by rogues and assassins to slip past guards or gain an advantage in stealthy situations.\\n3. Potion of Speed: This potion increases the drinker's speed for a short period of time, allowing them to move faster and cover more ground than normal. It's useful for races like halflings and gnomes, who already have high movement speeds, but can also be helpful for other classes that rely on mobility.\\n4. Potion of Strength: This potion enhances the drinker's physical strength, giving them extra muscle power and endurance. It's popular among barbarians and fighters, who use it to boost their damage output and survive longer in intense battles.\\n5. Potion of Teleportation: This rare and powerful potion allows the drinker to instantly transport themselves to a different location, potentially avoiding danger or reaching distant areas quickly. However, it requires careful preparation and can only be used once per day, making it a valuable resource for experienced adventurers.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### testing model, not using the harry potter books yet\n",
    "### answer is not necessarily related to harry potter\n",
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦œðŸ”— Langchain\n",
    "\n",
    "- Multiple document retriever with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama2-13b-chat'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader\n",
    "\n",
    "- [Directory loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory) for multiple files\n",
    "- This step is not necessary if you are just loading the vector database\n",
    "- This step is necessary if you are creating embeddings. In this case you need to:\n",
    "    - load de PDF files\n",
    "    - split into chunks\n",
    "    - create embeddings\n",
    "    - save the embeddings in a vector store\n",
    "    - After that you can just load the saved embeddings to do similarity search with the user query, and then use the LLM to answer the question\n",
    "    \n",
    "You can comment out this section if you use the embeddings I already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:14<00:00, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 440 ms, total: 1min 16s\n",
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    CFG.PDFs_path,\n",
    "    glob=\"./*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4114 pages in total\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {len(documents)} pages in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"8Ron\\nP.S. Percy's Head Boy. He got the letter last week.Harry glanced back at the photograph. Percy, who was in his seventh and\\nfinal year at Hogwarts, was looking particularly smug. He had pinned hisHead Boy badge to the fez perched jauntily on top of his neat hair, hishorn-rimmed glasses flashing in the Egyptian sun.\\nHarry now turned to his present and unwrapped it. Inside was what looked\\nlike a miniature glass spinning top. There was another note from Ronbeneath it.\\nHarry -- this is a Pocket Sneakoscope. If there's someone untrustworthy\\naround, it's supposed to light up and spin. Bill says it's rubbish soldfor wizard tourists and isn't reliable, because it kept lighting up atdinner last night. But he didn't realize Fred and George had put beetlesin his soup.\\nBye --RonHarry put the Pocket Sneakoscope on his bedside table, where it stood\\nquite still, balanced on its point, reflecting the luminous hands of hisclock. He looked at it happily for a few seconds, then picked up theparcel Hedwig had brought.\\nInside this, too, there was a wrapped present, a card, and a letter,\\nthis time from Hermione.\\nDear Harry,Ron wrote to me and told me about his phone call to your Uncle Vernon. I\\ndo hope you're all right.\\nI'm on holiday in France at the moment and I didn't know how I was going\\nto send this to you -- what if they'd opened it at customs? -- but thenHedwig turned up! I think she wanted to make sure you got something for\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[8].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter\n",
    "\n",
    "- Splitting the text into chunks so its passages are easily searchable for similarity\n",
    "- This step is also only necessary if you are creating the embeddings\n",
    "- [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/reference/modules/document_loaders.html?highlight=RecursiveCharacterTextSplitter#langchain.document_loaders.MWDumpLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have created 10519 chunks from 4114 pages\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = CFG.split_chunk_size,\n",
    "    chunk_overlap = CFG.split_overlap\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'We have created {len(texts)} chunks from {len(documents)} pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings\n",
    "\n",
    "\n",
    "- Embedd and store the texts in a Vector database (FAISS)\n",
    "- [LangChain Vector Stores docs](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "- [FAISS - langchain](https://python.langchain.com/docs/integrations/vectorstores/faiss)\n",
    "\n",
    "- [Chroma - Persist and load the vector database](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html)\n",
    "\n",
    "___\n",
    "\n",
    "- If you use Chroma vector store it will take ~35 min to create embeddings\n",
    "- If you use FAISS vector store on GPU it will take just ~3 min\n",
    "\n",
    "___\n",
    "\n",
    "We need to create the embeddings only once, and then we can just load the vector store and query the database using similarity search. \n",
    "\n",
    "Loading the embeddings takes only a few seconds.\n",
    "\n",
    "I uploaded the embeddings to a Kaggle Dataset so we just load it from [here](https://www.kaggle.com/datasets/hinepo/faiss-hp-sentence-transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load vector database\n",
    "\n",
    "- After saving the vector database, we just load it from the Kaggle Dataset I mentioned\n",
    "- Obviously, the embeddings function to load the embeddings must be the same as the one used to create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 366 ms, total: 1.38 s\n",
      "Wall time: 3.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### download embeddings model\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name = CFG.embeddings_model_repo,\n",
    "    model_kwargs = {\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "### load vector DB embeddings\n",
    "vectordb = FAISS.load_local(\n",
    "    CFG.Embeddings_path,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='â€œMagic?â€ he repeated in a whisper. \\nâ€œThatâ€™s right,â€ said Dumbledore. \\nâ€œItâ€™s â€¦ itâ€™s magic, what I can do?â€ \\nâ€œWhat is it that you can do?â€ \\nâ€œAll sorts,â€ breathed Riddle. A flush of excitement was \\nrising up his neck into his hollow cheeks; he looked \\nfevered. â€œI can make things move without touching \\nthem. I can make animals do what I want them to do, \\nwithout training them. I can make bad things happen \\nto people who annoy me. I can make them hurt if I \\nwant to.â€', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 6 - The Half-Blood Prince.pdf', 'page': 302}),\n",
       " Document(page_content='91\"Shut up, Malfoy,\" said Harry quietly. Hagrid was looking downcast and\\nHarry wanted Hagrid\\'s first lesson to be a success.\\n\"Righ\\' then,\" said Hagrid, who seemed to have lost his thread, \"so -- so\\nyeh\\'ve got yer books an\\' -- an\\' - - now yeh need the Magical Creatures.Yeah. So I\\'ll go an\\' get \\'em. Hang on... \"\\nHe strode away from them into the forest and out of sight.\"God, this place is going to the dogs,\" said Malfoy loudly. \"That oaf\\nteaching classes, my father\\'ll have a fit when I tell him\\n\"Shut up, Malfoy,\" Harry repeated.\"Careful, Potter, there\\'s a dementor behind you\"Oooooooh!\" squealed Lavender Brown, pointing toward the opposite side\\nof the paddock.\\nTrotting toward them were a dozen of the most bizarre creatures Harry', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 3 - The Prisoner of Azkaban.pdf', 'page': 91}),\n",
       " Document(page_content='says Draco Malfoy, a fourth-year student. â€œWe all hate Hagrid, but weâ€™re just too scared to say \\nanything.â€ \\nHagrid has no intention of ceasing his campaign \\nof intimidation, however. In conversation with a \\nDaily Prophet  reporter last month, he admitted \\nbreeding creatures he has dubbed â€œBlast-Ended \\nSkrewts,â€ highly dangerous crosses between manti-\\ncores and fire-crabs. The creation of new breeds of magical creature is, of course, an activity usually \\nclosely observed by the Department for the Regu-\\nlation and Control of Magical Creatures. Hagrid, however, considers himself to be above such petty \\nrestrictions.', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 4 - The Goblet of Fire.pdf', 'page': 453}),\n",
       " Document(page_content='Here and there adult wizards and witches were emerging from \\ntheir tents and starting to cook breakfast. Some, with furtive looks \\naround them, conjured fires with th eir wands; others were striking', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 4 - The Goblet of Fire.pdf', 'page': 96})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test if vector DB was loaded correctly\n",
    "vectordb.similarity_search('magic creatures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template\n",
    "\n",
    "- Custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_chain = LLMChain(prompt=PROMPT, llm=llm)\n",
    "# llm_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever chain\n",
    "\n",
    "- Retriever to retrieve relevant passages\n",
    "- Chain to answer questions\n",
    "- [RetrievalQA: Chain for question-answering](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='would warn Hagrid myself, but I am  banished â€” it would be unwise \\nfor me to go too near the forest now â€” Hagrid has troubles enough, \\nwithout a centaursâ€™ battle.â€ \\nâ€œBut â€” whatâ€™s Hagrid attempting to do?â€ said Harry nervously. \\nFirenze looked at Harry impassively. \\nâ€œHagrid has recently rendered me a great service,â€ said Firenze,', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 5 - The Order of the Phoenix.pdf', 'page': 619}),\n",
       " Document(page_content=\"wisely. Behind him, Buckbeak spat a few ferret bones onto Hagrid'spillow.\", metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 3 - The Prisoner of Azkaban.pdf', 'page': 228}),\n",
       " Document(page_content='says Draco Malfoy, a fourth-year student. â€œWe all hate Hagrid, but weâ€™re just too scared to say \\nanything.â€ \\nHagrid has no intention of ceasing his campaign \\nof intimidation, however. In conversation with a \\nDaily Prophet  reporter last month, he admitted \\nbreeding creatures he has dubbed â€œBlast-Ended \\nSkrewts,â€ highly dangerous crosses between manti-\\ncores and fire-crabs. The creation of new breeds of magical creature is, of course, an activity usually \\nclosely observed by the Department for the Regu-\\nlation and Control of Magical Creatures. Hagrid, however, considers himself to be above such petty \\nrestrictions.', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 4 - The Goblet of Fire.pdf', 'page': 453})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing MMR search\n",
    "question = \"Which are Hagrid's favorite animals?\"\n",
    "vectordb.max_marginal_relevance_search(question, k = CFG.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='would warn Hagrid myself, but I am  banished â€” it would be unwise \\nfor me to go too near the forest now â€” Hagrid has troubles enough, \\nwithout a centaursâ€™ battle.â€ \\nâ€œBut â€” whatâ€™s Hagrid attempting to do?â€ said Harry nervously. \\nFirenze looked at Harry impassively. \\nâ€œHagrid has recently rendered me a great service,â€ said Firenze,', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 5 - The Order of the Phoenix.pdf', 'page': 619}),\n",
       " Document(page_content=\"Harry could sort of see what Hagrid meant. Once you got over the first\\nshock of seeing something that was, half horse, half bird, you startedto appreciate the hippogriffs' gleaming coats, changing smoothly fromfeather to hair, each of them a different color: stormy gray, bronze,\", metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 3 - The Prisoner of Azkaban.pdf', 'page': 91}),\n",
       " Document(page_content='CHAPTER  THIRTEEN \\n\\x91 198 \\x91 nothing better than a pet drag on, as Harry, Ron, and Hermione \\nknew only too well â€” he had owned one for a brief period during \\ntheir first year, a vicious Norweg ian Ridgeback by the name of \\nNorbert. Hagrid simply loved monstrous creatures, the more \\nlethal, the better. \\nâ€œWell, at least the skrewts are sma ll,â€ said Ron as they made their \\nway back up to the castle for lunch an hour later. \\nâ€œThey are now, â€ said Hermione in an exasperated voice, â€œbut \\nonce Hagridâ€™s found out what they eat, I expect theyâ€™ll be six feet \\nlong.â€ \\nâ€œWell, that wonâ€™t matter if they turn out to cure seasickness or \\nsomething, will it?â€ said Ro n, grinning slyly at her. \\nâ€œYou know perfectly we ll I only said that to shut Malfoy up,â€', metadata={'source': '/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/Harry Potter - Book 4 - The Goblet of Fire.pdf', 'page': 213})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing similarity search\n",
    "question = \"Which are Hagrid's favorite animals?\"\n",
    "vectordb.similarity_search(question, k = CFG.k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process outputs\n",
    "\n",
    "- Format llm response\n",
    "- Cite sources (PDFs)\n",
    "- Change `width` parameter to format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=700):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "    \n",
    "    sources_used = ' \\n'.join(\n",
    "        [\n",
    "            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])\n",
    "            for source in llm_response['source_documents']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    \n",
    "    llm_response = qa_chain.invoke(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans + time_elapsed_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask questions\n",
    "\n",
    "- Question Answering from multiple documents\n",
    "- Invoke QA Chain\n",
    "- Talk to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama2-13b-chat'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " During the Triwizard Tournament, Harry faces challenges such as the Merpeople, the Maze, and the final task where he has to retrieve a golden egg from a dragon.\n",
      "\n",
      "Sources: \n",
      "Harry Potter - Book 4 - The Goblet of Fire - page: 271 \n",
      "Harry Potter - Book 4 - The Goblet of Fire - page: 237 \n",
      "Harry Potter - Book 4 - The Goblet of Fire - page: 252\n",
      "\n",
      "Time elapsed: 7 s\n"
     ]
    }
   ],
   "source": [
    "query = \"Which challenges does Harry face during the Triwizard Tournament?\"\n",
    "print(llm_ans(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the passage, it appears that Lucius Malfoy is a servant or follower of Voldemort, rather than an ally. The passage states that Voldemort has \"disappointed\" Malfoy in the past, and that Malfoy is afraid of Voldemort and tries to curry favor with him. Additionally, the fact that Voldemort refers to Malfoy as \"Lucius\" and not by a title or name suggests that he does not consider Malfoy to be an equal or a true ally.\n",
      "\n",
      "Sources: \n",
      "Harry Potter - Book 6 - The Half-Blood Prince - page: 716 \n",
      "Harry Potter - Book 4 - The Goblet of Fire - page: 665 \n",
      "Harry Potter - Book 7 - The Deathly Hallows - page: 16\n",
      "\n",
      "Time elapsed: 21 s\n"
     ]
    }
   ],
   "source": [
    "query = \"Is Malfoy an ally of Voldemort?\"\n",
    "print(llm_ans(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An object in which a person has concealed part of their soul.\n",
      "\n",
      "Sources: \n",
      "Harry Potter - Book 6 - The Half-Blood Prince - page: 557 \n",
      "Harry Potter - Book 6 - The Half-Blood Prince - page: 419 \n",
      "Harry Potter - Book 6 - The Half-Blood Prince - page: 715\n",
      "\n",
      "Time elapsed: 4 s\n"
     ]
    }
   ],
   "source": [
    "query = \"What are horcrux?\"\n",
    "print(llm_ans(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are many cool potions described throughout the series but here are five examples:\n",
      "1) Polyjuice Potion - allows the drinker to transform into another person (or people).\n",
      "2) Felix Felicis - also known as Liquid Luck, this potion causes the drinker to experience good fortune and luck.\n",
      "3) Amortentia - a love potion that smells differently depending on the person who drinks it.\n",
      "4) Draught of Living Death - a powerful potion that puts the drinker into a deep coma, making them appear dead.\n",
      "5) Venomous Tentacula Strength-increasing draught - increases the strength of the drinker, allowing them to perform feats of physical prowess.\n",
      "\n",
      "Sources: \n",
      "Harry Potter - Book 6 - The Half-Blood Prince - page: 204 \n",
      "Harry Potter - Book 6 - The Half-Blood Prince - page: 204 \n",
      "Harry Potter - Book 7 - The Deathly Hallows - page: 76\n",
      "\n",
      "Time elapsed: 27 s\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "print(llm_ans(query))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3327967,
     "sourceId": 5793551,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3669776,
     "sourceId": 6369513,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30498,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
